# RL & Alignment for LLMs

> Parent: [AI Infrastructure](../00_AI_Infra.md)

## Overview

Reinforcement Learning from Human Feedback (RLHF) and related alignment techniques are essential for making LLMs helpful, harmless, and honest. This phase covers the algorithms, infrastructure, and frameworks for aligning language models with human preferences.

## The Alignment Pipeline

```
┌─────────────────────────────────────────────────────────────────┐
│                    LLM Alignment Pipeline                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  1. Pre-training          2. SFT              3. Alignment       │
│  ┌──────────────┐      ┌──────────────┐      ┌──────────────┐   │
│  │ Next-token   │ ───▶ │ Supervised   │ ───▶ │ RLHF / DPO   │   │
│  │ prediction   │      │ Fine-tuning  │      │ / GRPO       │   │
│  │ on web data  │      │ on instruct  │      │              │   │
│  └──────────────┘      └──────────────┘      └──────────────┘   │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

## Why Alignment Matters

Pre-trained LLMs learn to predict text, not to be helpful. They may:
- Generate harmful or biased content
- Refuse to follow instructions
- Produce verbose or unhelpful responses
- Hallucinate confidently

Alignment techniques teach models to:
- Follow user intent
- Refuse harmful requests appropriately
- Be concise and helpful
- Acknowledge uncertainty

## Learning Path

| Topic | File | Priority |
|-------|------|----------|
| Alignment Algorithms | [01_Algorithms.md](01_Algorithms.md) | ★★★★★ |
| RL Infrastructure | [02_Infrastructure.md](02_Infrastructure.md) | ★★★★☆ |
| Frameworks | [03_Frameworks.md](03_Frameworks.md) | ★★★★☆ |

### Algorithm Coverage in 01_Algorithms.md

| Algorithm | Year | Key Innovation |
|-----------|------|----------------|
| PPO (RLHF) | 2017/2022 | Clipped objective, 4-model setup |
| DPO | 2023 | Direct preference, no reward model |
| IPO, KTO, ORPO | 2023-2024 | DPO variants |
| GRPO | 2024 | Group normalization, no value model |
| RLOO | 2024 | Leave-one-out baseline |
| GDPO | 2025 | Distributional preferences |

---

## The Big Picture: Alignment Methods

```
                           Alignment Methods
                                  │
        ┌─────────────────────────┼─────────────────────────┐
        ▼                         ▼                         ▼
┌──────────────┐          ┌──────────────┐          ┌──────────────┐
│     RLHF     │          │  Preference  │          │    Online    │
│  (PPO-based) │          │   Methods    │          │   Methods    │
└──────────────┘          └──────────────┘          └──────────────┘
       │                         │                         │
┌──────┴──────┐           ┌──────┴──────┐           ┌──────┴──────┐
│• Reward     │           │• DPO        │           │• GRPO       │
│  model      │           │• IPO, KTO   │           │• RLOO       │
│• PPO optim  │           │• ORPO, SimPO│           │• Online DPO │
│• 4 models   │           │• GDPO       │           │• DeepSeek-R1│
│  in memory  │           │  (2025)     │           │             │
└─────────────┘           └─────────────┘           └─────────────┘
```

### Algorithm Timeline

```
2017        2022           2023         2024           2025
 │           │              │            │              │
 ▼           ▼              ▼            ▼              ▼
PPO ──▶ InstructGPT ──▶   DPO    ──▶  GRPO   ──▶    GDPO
        (RLHF)         (reward-free)  (DeepSeek)  (pluralistic)
                           │                          │
                      IPO, KTO,                  DeepSeek-R1
                      ORPO, SimPO
```

## Key Papers

| Paper | Year | Key Contribution |
|-------|------|------------------|
| [InstructGPT (OpenAI)](https://arxiv.org/abs/2203.02155) | 2022 | RLHF pipeline for instruction following |
| [Constitutional AI (Anthropic)](https://arxiv.org/abs/2212.08073) | 2022 | RLAIF, self-improvement |
| [DPO](https://arxiv.org/abs/2305.18290) | 2023 | Direct preference optimization without RM |
| [IPO](https://arxiv.org/abs/2310.12036) | 2023 | Identity preference optimization |
| [KTO](https://arxiv.org/abs/2402.01306) | 2024 | Kahneman-Tversky optimization |
| [GRPO (DeepSeek)](https://arxiv.org/abs/2402.03300) | 2024 | Group relative policy optimization |
| [DeepSeek-R1](https://arxiv.org/abs/2501.12948) | 2025 | Pure RL without SFT, reasoning emergence |
| [GDPO](https://arxiv.org/abs/2412.20299) | 2025 | Distributional preferences, pluralistic alignment |

## Key Blogs & Resources

- [HuggingFace RLHF Blog](https://huggingface.co/blog/rlhf)
- [Anthropic RLHF Documentation](https://www.anthropic.com/research)
- [OpenAI Alignment Research](https://openai.com/research/alignment)
- [TRL Documentation](https://huggingface.co/docs/trl)

---

## Notes

### RLHF vs DPO: Quick Comparison

| Aspect | RLHF (PPO) | DPO |
|--------|------------|-----|
| Reward model | Required | Not needed |
| Training stability | Tricky | More stable |
| Computational cost | High (4 models) | Lower (2 models) |
| Online learning | Yes | No (offline) |
| Sample efficiency | Lower | Higher |
| Hyperparameter sensitivity | High | Lower |

### The Four Models in RLHF

```
┌─────────────────────────────────────────────────────────────┐
│                    RLHF Training Setup                       │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  1. Policy Model (π_θ)      ←── Being optimized              │
│     └── Generates responses                                  │
│                                                              │
│  2. Reference Model (π_ref) ←── Frozen copy of initial SFT   │
│     └── KL penalty anchor                                    │
│                                                              │
│  3. Reward Model (r_φ)      ←── Trained on preferences       │
│     └── Scores responses                                     │
│                                                              │
│  4. Value Model (V_ψ)       ←── Estimates expected reward    │
│     └── Reduces variance in PPO                              │
│                                                              │
└─────────────────────────────────────────────────────────────┘

Memory: 4 × model_size (can be prohibitive for large models!)
```

### The DPO Insight

DPO shows that the optimal policy under RLHF can be expressed directly:

```
π*(y|x) ∝ π_ref(y|x) · exp(r(x,y) / β)
```

This means we can skip the reward model and optimize directly on preferences:

```
                    RLHF                          DPO
                      │                             │
    ┌─────────────────┼─────────────────┐          │
    ▼                 ▼                 ▼          ▼
┌────────┐      ┌────────┐      ┌────────┐    ┌────────┐
│Collect │      │ Train  │      │  PPO   │    │Direct  │
│Prefs   │ ───▶ │ Reward │ ───▶ │Training│    │Optimize│
│(x,y+,y-)│     │ Model  │      │        │    │on Prefs│
└────────┘      └────────┘      └────────┘    └────────┘
                    ↑                              │
                    └──── Skip this! ──────────────┘
```

### Preference Data Format

All methods need preference data in this format:

```python
{
    "prompt": "Explain quantum computing",
    "chosen": "Quantum computing uses quantum mechanical phenomena...",  # Better response
    "rejected": "Quantum computing is a type of computing..."  # Worse response
}
```

Sources of preference data:
1. **Human labelers**: Gold standard, expensive
2. **AI feedback**: Use stronger model to judge (RLAIF)
3. **Synthetic**: Generate using rules/heuristics
4. **Implicit**: User behavior (thumbs up/down, regenerate)

### Training Objectives Comparison

**RLHF (PPO):**
```
max_θ E[r(x,y) - β·KL(π_θ || π_ref)]
```

**DPO:**
```
min_θ -E[log σ(β·log(π_θ(y+|x)/π_ref(y+|x)) - β·log(π_θ(y-|x)/π_ref(y-|x)))]
```

**GRPO:**
```
max_θ E[A(x,y) · log π_θ(y|x) - β·KL(π_θ || π_ref)]
where A(x,y) = (r(x,y) - mean(r)) / std(r)  # Group-normalized advantage
```

### When to Use What

| Scenario | Recommended Method | Why |
|----------|-------------------|-----|
| First alignment attempt | DPO | Simpler, stable |
| Large-scale production | RLHF | Better ceiling |
| Limited compute | DPO or KTO | Less overhead |
| Reasoning tasks | GRPO | Works well for math/code |
| Continuous improvement | Online DPO | Iterative refinement |
| Safety alignment | Constitutional AI | Scalable oversight |
